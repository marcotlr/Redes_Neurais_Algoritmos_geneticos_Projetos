{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6b95101-9cbc-4fa8-a9ba-7e1690f47e0b",
   "metadata": {},
   "source": [
    "## 4.2 Stop right now, thank you very much\n",
    "\n",
    "#### Objetivo: \n",
    "implemente uma estratégia de Parada Antecipada (Early Stopping) no\n",
    "processo de treino da rede neural feita em Python puro ou no processo de treino da rede\n",
    "neural feita em PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb365ab1",
   "metadata": {},
   "source": [
    "## Introdução\n",
    "\n",
    "O processo de treinamento de uma rede neural costuma envolver a otimização iterativa dos pesos até um número máximo de épocas seja alcançado. No entanto, treinar por muitas épocas pode levar a **overfitting**, que é quando o modelo começa a ajustar-se demais aos ruídos dos dados de treinamento e perde capacidade de generalização nos exemplos novos. Uma estratégia simples e eficiente para contornar esse problema é a **Parada Antecipada (Early Stopping)**, que interrompe o treino tão logo a performance em um conjunto de validação deixe de melhorar.\n",
    "\n",
    "Neste notebook implementamos o Early Stopping em uma rede neural feita no Pytorch, aproveitando o mecanismo de `torch.nn` e `torch.optim`.  \n",
    "Para esse trabalho, vamos usar um dataset associado a dados cardíacos onde será feito um tratamento dos dados(retirada de colunas que não são funcionais, normalização dos dados e divisão em dados de treino, validação e teste). Além disso, será necessários converter os dados em Tensores, que é uma necessidade para usar o Pytotch.\n",
    "Vamos monitorar a **loss** no conjunto de validação a cada época, salvar o melhor estado de parâmetros e interromper o treino quando não houver melhora após um número fixo de épocas (paciencia), garantindo que não ocorra overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13eae94c-96c0-4f39-9bb4-30d58cd93601",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marco24038\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91262595",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_dados_entrada, neuronios_c1, neuronios_c2, num_targets):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.camadas = nn.Sequential(\n",
    "            nn.Linear(num_dados_entrada, neuronios_c1),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(neuronios_c1, neuronios_c2),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(neuronios_c2, num_targets),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.camadas(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f52abc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Heart rate</th>\n",
       "      <th>Systolic blood pressure</th>\n",
       "      <th>Diastolic blood pressure</th>\n",
       "      <th>Blood sugar</th>\n",
       "      <th>CK-MB</th>\n",
       "      <th>Troponin</th>\n",
       "      <th>Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>66</td>\n",
       "      <td>160</td>\n",
       "      <td>83</td>\n",
       "      <td>160.0</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.012</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>94</td>\n",
       "      <td>98</td>\n",
       "      <td>46</td>\n",
       "      <td>296.0</td>\n",
       "      <td>6.75</td>\n",
       "      <td>1.060</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>160</td>\n",
       "      <td>77</td>\n",
       "      <td>270.0</td>\n",
       "      <td>1.99</td>\n",
       "      <td>0.003</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>70</td>\n",
       "      <td>120</td>\n",
       "      <td>55</td>\n",
       "      <td>270.0</td>\n",
       "      <td>13.87</td>\n",
       "      <td>0.122</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>112</td>\n",
       "      <td>65</td>\n",
       "      <td>300.0</td>\n",
       "      <td>1.08</td>\n",
       "      <td>0.003</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1314</th>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>94</td>\n",
       "      <td>122</td>\n",
       "      <td>67</td>\n",
       "      <td>204.0</td>\n",
       "      <td>1.63</td>\n",
       "      <td>0.006</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1315</th>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>84</td>\n",
       "      <td>125</td>\n",
       "      <td>55</td>\n",
       "      <td>149.0</td>\n",
       "      <td>1.33</td>\n",
       "      <td>0.172</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1316</th>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>168</td>\n",
       "      <td>104</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.24</td>\n",
       "      <td>4.250</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1317</th>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>117</td>\n",
       "      <td>68</td>\n",
       "      <td>443.0</td>\n",
       "      <td>5.80</td>\n",
       "      <td>0.359</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1318</th>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>94</td>\n",
       "      <td>157</td>\n",
       "      <td>79</td>\n",
       "      <td>134.0</td>\n",
       "      <td>50.89</td>\n",
       "      <td>1.770</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1319 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Age  Gender  Heart rate  Systolic blood pressure  \\\n",
       "0      64       1          66                      160   \n",
       "1      21       1          94                       98   \n",
       "2      55       1          64                      160   \n",
       "3      64       1          70                      120   \n",
       "4      55       1          64                      112   \n",
       "...   ...     ...         ...                      ...   \n",
       "1314   44       1          94                      122   \n",
       "1315   66       1          84                      125   \n",
       "1316   45       1          85                      168   \n",
       "1317   54       1          58                      117   \n",
       "1318   51       1          94                      157   \n",
       "\n",
       "      Diastolic blood pressure  Blood sugar  CK-MB  Troponin    Result  \n",
       "0                           83        160.0   1.80     0.012  negative  \n",
       "1                           46        296.0   6.75     1.060  positive  \n",
       "2                           77        270.0   1.99     0.003  negative  \n",
       "3                           55        270.0  13.87     0.122  positive  \n",
       "4                           65        300.0   1.08     0.003  negative  \n",
       "...                        ...          ...    ...       ...       ...  \n",
       "1314                        67        204.0   1.63     0.006  negative  \n",
       "1315                        55        149.0   1.33     0.172  positive  \n",
       "1316                       104         96.0   1.24     4.250  positive  \n",
       "1317                        68        443.0   5.80     0.359  positive  \n",
       "1318                        79        134.0  50.89     1.770  positive  \n",
       "\n",
       "[1319 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv(\"C:/Users/marco24038/OneDrive - ILUM ESCOLA DE CIÊNCIA/Redes Neurais e Algoritmos Genéticos/Redes_Neurais_Trabalhos/Feras formidáveis/Fera 4.2/Medicaldataset.csv\")\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e0d8010",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"Blood sugar\", \"Result\", \"Gender\"])\n",
    "y = df[\"Blood sugar\"].values.reshape(-1, 1)\n",
    "\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa37d1dd-b051-4ea8-999f-5e27d6ff0b7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.29050602e-02],\n",
       "       [-7.37248828e-01],\n",
       "       [-4.92186322e-01],\n",
       "       [-1.92665481e-01],\n",
       "       [-6.41946742e-01],\n",
       "       [ 1.88542862e-01],\n",
       "       [-6.41946742e-01],\n",
       "       [-6.41946742e-01],\n",
       "       [-7.78092579e-01],\n",
       "       [-8.87009248e-01],\n",
       "       [ 3.74194920e+00],\n",
       "       [-5.19415489e-01],\n",
       "       [ 8.69272046e-01],\n",
       "       [ 1.07349080e+00],\n",
       "       [-5.19415489e-01],\n",
       "       [-4.92186322e-01],\n",
       "       [-5.60259240e-01],\n",
       "       [-6.28332158e-01],\n",
       "       [ 5.36208466e+00],\n",
       "       [ 2.56615781e-01],\n",
       "       [-4.10498820e-01],\n",
       "       [-5.33030073e-01],\n",
       "       [ 3.33351169e+00],\n",
       "       [ 6.78667875e-01],\n",
       "       [-6.41946742e-01],\n",
       "       [-1.38207146e-01],\n",
       "       [-7.50863412e-01],\n",
       "       [-5.87488407e-01],\n",
       "       [-6.41946742e-01],\n",
       "       [-7.50863412e-01],\n",
       "       [-6.14717575e-01],\n",
       "       [-5.19415489e-01],\n",
       "       [-6.82790493e-01],\n",
       "       [-4.10498820e-01],\n",
       "       [-2.06130916e-03],\n",
       "       [ 3.79147034e-01],\n",
       "       [-4.37727987e-01],\n",
       "       [-6.96405077e-01],\n",
       "       [-7.23634244e-01],\n",
       "       [-5.46644656e-01],\n",
       "       [-3.01582150e-01],\n",
       "       [ 1.00541788e+00],\n",
       "       [ 6.78667875e-01],\n",
       "       [ 5.28907454e-01],\n",
       "       [-7.37248828e-01],\n",
       "       [ 9.37344965e-01],\n",
       "       [-5.19415489e-01],\n",
       "       [-2.33509232e-01],\n",
       "       [-4.64957154e-01],\n",
       "       [ 6.92282459e-01],\n",
       "       [-7.37248828e-01],\n",
       "       [-8.37488112e-02],\n",
       "       [-4.51342571e-01],\n",
       "       [-6.96405077e-01],\n",
       "       [-5.60259240e-01],\n",
       "       [-3.42425901e-01],\n",
       "       [-7.23634244e-01],\n",
       "       [ 3.64664712e+00],\n",
       "       [-7.50863412e-01],\n",
       "       [-2.92904765e-02],\n",
       "       [ 1.25048039e+00],\n",
       "       [ 3.24688699e-01],\n",
       "       [-7.01342276e-02],\n",
       "       [ 3.90532421e+00],\n",
       "       [ 6.51438708e-01],\n",
       "       [ 9.32407766e-02],\n",
       "       [-6.55561326e-01],\n",
       "       [-6.55561326e-01],\n",
       "       [ 7.19511626e-01],\n",
       "       [-8.18936330e-01],\n",
       "       [-2.92904765e-02],\n",
       "       [-6.14717575e-01],\n",
       "       [-7.64477995e-01],\n",
       "       [-3.83269652e-01],\n",
       "       [-6.82790493e-01],\n",
       "       [-6.82790493e-01],\n",
       "       [-6.28332158e-01],\n",
       "       [-7.10019661e-01],\n",
       "       [ 2.29386613e-01],\n",
       "       [-6.82790493e-01],\n",
       "       [-5.05800905e-01],\n",
       "       [ 1.12794914e+00],\n",
       "       [-6.55561326e-01],\n",
       "       [-7.10019661e-01],\n",
       "       [-5.46644656e-01],\n",
       "       [-4.92186322e-01],\n",
       "       [-6.41946742e-01],\n",
       "       [ 9.50959549e-01],\n",
       "       [ 1.10071997e+00],\n",
       "       [-6.28332158e-01],\n",
       "       [ 8.55657463e-01],\n",
       "       [ 3.00676168e+00],\n",
       "       [-4.37727987e-01],\n",
       "       [-5.73873824e-01],\n",
       "       [-8.05321746e-01],\n",
       "       [-9.68696751e-01],\n",
       "       [ 1.34578248e+00],\n",
       "       [-6.82790493e-01],\n",
       "       [ 5.15292871e-01],\n",
       "       [-3.83269652e-01],\n",
       "       [-6.69175910e-01],\n",
       "       [-2.33509232e-01],\n",
       "       [-1.92665481e-01],\n",
       "       [ 3.19736586e+00],\n",
       "       [-1.10484259e+00],\n",
       "       [ 8.28428295e-01],\n",
       "       [-1.79050897e-01],\n",
       "       [-9.00623832e-01],\n",
       "       [ 8.01199128e-01],\n",
       "       [-1.18653009e+00],\n",
       "       [-6.01102991e-01],\n",
       "       [-6.69175910e-01],\n",
       "       [-6.69175910e-01],\n",
       "       [-3.56040485e-01],\n",
       "       [ 2.72085543e+00],\n",
       "       [ 1.01903247e+00],\n",
       "       [ 2.43001197e-01],\n",
       "       [-7.50863412e-01],\n",
       "       [-4.92186322e-01],\n",
       "       [-8.18936330e-01],\n",
       "       [-2.06280064e-01],\n",
       "       [-8.18936330e-01],\n",
       "       [-3.28811318e-01],\n",
       "       [ 1.27770956e+00],\n",
       "       [ 9.78188716e-01],\n",
       "       [-7.37248828e-01],\n",
       "       [ 1.31855331e+00],\n",
       "       [-8.46165497e-01],\n",
       "       [-1.51821730e-01],\n",
       "       [-1.79050897e-01],\n",
       "       [ 1.98566791e+00],\n",
       "       [-8.46165497e-01]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_train = scaler_X.fit_transform(X_train)\n",
    "X_val = scaler_X.transform(X_val)\n",
    "X_test = scaler_X.transform(X_test)\n",
    "\n",
    "y_train = scaler_y.fit_transform(y_train)\n",
    "y_val = scaler_y.transform(y_val)\n",
    "y_test = scaler_y.transform(y_test)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e991c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a453bced",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_DADOS_DE_ENTRADA = X_train.shape[1] \n",
    "NUM_DADOS_DE_SAIDA = 1  \n",
    "NEURONIOS_C1 = 5  \n",
    "NEURONIOS_C2 = 3  \n",
    "\n",
    "minha_mlp = MLP(\n",
    "    NUM_DADOS_DE_ENTRADA, NEURONIOS_C1, NEURONIOS_C2, NUM_DADOS_DE_SAIDA\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "708e2cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAXA_DE_APRENDIZADO = 0.001\n",
    "otimizador = optim.SGD(minha_mlp.parameters(), lr=TAXA_DE_APRENDIZADO)\n",
    "\n",
    "\n",
    "fn_perda = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a65f71fe-ec17-4203-b899-13ef84544f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época 0: perda treino = 1.2303, perda validação = 1.2723\n",
      "Época 1: perda treino = 1.2284, perda validação = 1.2708\n",
      "Época 2: perda treino = 1.2266, perda validação = 1.2693\n",
      "Época 3: perda treino = 1.2248, perda validação = 1.2679\n",
      "Época 4: perda treino = 1.2230, perda validação = 1.2665\n",
      "Época 5: perda treino = 1.2213, perda validação = 1.2650\n",
      "Época 6: perda treino = 1.2195, perda validação = 1.2636\n",
      "Época 7: perda treino = 1.2178, perda validação = 1.2622\n",
      "Época 8: perda treino = 1.2160, perda validação = 1.2609\n",
      "Época 9: perda treino = 1.2143, perda validação = 1.2595\n",
      "Época 10: perda treino = 1.2126, perda validação = 1.2581\n",
      "Época 11: perda treino = 1.2109, perda validação = 1.2568\n",
      "Época 12: perda treino = 1.2093, perda validação = 1.2555\n",
      "Época 13: perda treino = 1.2076, perda validação = 1.2541\n",
      "Época 14: perda treino = 1.2059, perda validação = 1.2528\n",
      "Época 15: perda treino = 1.2043, perda validação = 1.2515\n",
      "Época 16: perda treino = 1.2027, perda validação = 1.2502\n",
      "Época 17: perda treino = 1.2011, perda validação = 1.2490\n",
      "Época 18: perda treino = 1.1995, perda validação = 1.2477\n",
      "Época 19: perda treino = 1.1979, perda validação = 1.2465\n",
      "Época 20: perda treino = 1.1963, perda validação = 1.2452\n",
      "Época 21: perda treino = 1.1948, perda validação = 1.2440\n",
      "Época 22: perda treino = 1.1932, perda validação = 1.2428\n",
      "Época 23: perda treino = 1.1917, perda validação = 1.2416\n",
      "Época 24: perda treino = 1.1902, perda validação = 1.2404\n",
      "Época 25: perda treino = 1.1887, perda validação = 1.2392\n",
      "Época 26: perda treino = 1.1872, perda validação = 1.2380\n",
      "Época 27: perda treino = 1.1857, perda validação = 1.2368\n",
      "Época 28: perda treino = 1.1842, perda validação = 1.2357\n",
      "Época 29: perda treino = 1.1827, perda validação = 1.2346\n",
      "Época 30: perda treino = 1.1813, perda validação = 1.2334\n",
      "Época 31: perda treino = 1.1799, perda validação = 1.2323\n",
      "Época 32: perda treino = 1.1784, perda validação = 1.2312\n",
      "Época 33: perda treino = 1.1770, perda validação = 1.2301\n",
      "Época 34: perda treino = 1.1756, perda validação = 1.2290\n",
      "Época 35: perda treino = 1.1742, perda validação = 1.2279\n",
      "Época 36: perda treino = 1.1728, perda validação = 1.2268\n",
      "Época 37: perda treino = 1.1715, perda validação = 1.2258\n",
      "Época 38: perda treino = 1.1701, perda validação = 1.2247\n",
      "Época 39: perda treino = 1.1688, perda validação = 1.2237\n",
      "Época 40: perda treino = 1.1674, perda validação = 1.2227\n",
      "Época 41: perda treino = 1.1661, perda validação = 1.2216\n",
      "Época 42: perda treino = 1.1648, perda validação = 1.2206\n",
      "Época 43: perda treino = 1.1635, perda validação = 1.2196\n",
      "Época 44: perda treino = 1.1622, perda validação = 1.2186\n",
      "Época 45: perda treino = 1.1609, perda validação = 1.2176\n",
      "Época 46: perda treino = 1.1596, perda validação = 1.2167\n",
      "Época 47: perda treino = 1.1584, perda validação = 1.2157\n",
      "Época 48: perda treino = 1.1571, perda validação = 1.2147\n",
      "Época 49: perda treino = 1.1559, perda validação = 1.2138\n",
      "Época 50: perda treino = 1.1546, perda validação = 1.2129\n",
      "Época 51: perda treino = 1.1534, perda validação = 1.2119\n",
      "Época 52: perda treino = 1.1522, perda validação = 1.2110\n",
      "Época 53: perda treino = 1.1510, perda validação = 1.2101\n",
      "Época 54: perda treino = 1.1498, perda validação = 1.2092\n",
      "Época 55: perda treino = 1.1486, perda validação = 1.2083\n",
      "Época 56: perda treino = 1.1474, perda validação = 1.2074\n",
      "Época 57: perda treino = 1.1463, perda validação = 1.2065\n",
      "Época 58: perda treino = 1.1451, perda validação = 1.2056\n",
      "Época 59: perda treino = 1.1440, perda validação = 1.2048\n",
      "Época 60: perda treino = 1.1428, perda validação = 1.2039\n",
      "Época 61: perda treino = 1.1417, perda validação = 1.2031\n",
      "Época 62: perda treino = 1.1406, perda validação = 1.2022\n",
      "Época 63: perda treino = 1.1395, perda validação = 1.2014\n",
      "Época 64: perda treino = 1.1383, perda validação = 1.2006\n",
      "Época 65: perda treino = 1.1373, perda validação = 1.1997\n",
      "Época 66: perda treino = 1.1362, perda validação = 1.1989\n",
      "Época 67: perda treino = 1.1351, perda validação = 1.1981\n",
      "Época 68: perda treino = 1.1340, perda validação = 1.1973\n",
      "Época 69: perda treino = 1.1330, perda validação = 1.1965\n",
      "Época 70: perda treino = 1.1319, perda validação = 1.1958\n",
      "Época 71: perda treino = 1.1309, perda validação = 1.1950\n",
      "Época 72: perda treino = 1.1298, perda validação = 1.1942\n",
      "Época 73: perda treino = 1.1288, perda validação = 1.1935\n",
      "Época 74: perda treino = 1.1278, perda validação = 1.1927\n",
      "Época 75: perda treino = 1.1268, perda validação = 1.1920\n",
      "Época 76: perda treino = 1.1258, perda validação = 1.1912\n",
      "Época 77: perda treino = 1.1248, perda validação = 1.1905\n",
      "Época 78: perda treino = 1.1238, perda validação = 1.1898\n",
      "Época 79: perda treino = 1.1228, perda validação = 1.1891\n",
      "Época 80: perda treino = 1.1218, perda validação = 1.1883\n",
      "Época 81: perda treino = 1.1209, perda validação = 1.1876\n",
      "Época 82: perda treino = 1.1199, perda validação = 1.1869\n",
      "Época 83: perda treino = 1.1190, perda validação = 1.1863\n",
      "Época 84: perda treino = 1.1180, perda validação = 1.1856\n",
      "Época 85: perda treino = 1.1171, perda validação = 1.1849\n",
      "Época 86: perda treino = 1.1162, perda validação = 1.1842\n",
      "Época 87: perda treino = 1.1153, perda validação = 1.1836\n",
      "Época 88: perda treino = 1.1143, perda validação = 1.1829\n",
      "Época 89: perda treino = 1.1134, perda validação = 1.1822\n",
      "Época 90: perda treino = 1.1125, perda validação = 1.1816\n",
      "Época 91: perda treino = 1.1117, perda validação = 1.1810\n",
      "Época 92: perda treino = 1.1108, perda validação = 1.1803\n",
      "Época 93: perda treino = 1.1099, perda validação = 1.1797\n",
      "Época 94: perda treino = 1.1090, perda validação = 1.1791\n",
      "Época 95: perda treino = 1.1082, perda validação = 1.1785\n",
      "Época 96: perda treino = 1.1073, perda validação = 1.1778\n",
      "Época 97: perda treino = 1.1065, perda validação = 1.1772\n",
      "Época 98: perda treino = 1.1056, perda validação = 1.1766\n",
      "Época 99: perda treino = 1.1048, perda validação = 1.1760\n",
      "Época 100: perda treino = 1.1040, perda validação = 1.1755\n",
      "Época 101: perda treino = 1.1031, perda validação = 1.1749\n",
      "Época 102: perda treino = 1.1023, perda validação = 1.1743\n",
      "Época 103: perda treino = 1.1015, perda validação = 1.1737\n",
      "Época 104: perda treino = 1.1007, perda validação = 1.1732\n",
      "Época 105: perda treino = 1.0999, perda validação = 1.1726\n",
      "Época 106: perda treino = 1.0991, perda validação = 1.1721\n",
      "Época 107: perda treino = 1.0984, perda validação = 1.1715\n",
      "Época 108: perda treino = 1.0976, perda validação = 1.1710\n",
      "Época 109: perda treino = 1.0968, perda validação = 1.1704\n",
      "Época 110: perda treino = 1.0960, perda validação = 1.1699\n",
      "Época 111: perda treino = 1.0953, perda validação = 1.1694\n",
      "Época 112: perda treino = 1.0945, perda validação = 1.1688\n",
      "Época 113: perda treino = 1.0938, perda validação = 1.1683\n",
      "Época 114: perda treino = 1.0930, perda validação = 1.1678\n",
      "Época 115: perda treino = 1.0923, perda validação = 1.1673\n",
      "Época 116: perda treino = 1.0916, perda validação = 1.1668\n",
      "Época 117: perda treino = 1.0909, perda validação = 1.1663\n",
      "Época 118: perda treino = 1.0901, perda validação = 1.1658\n",
      "Época 119: perda treino = 1.0894, perda validação = 1.1653\n",
      "Época 120: perda treino = 1.0887, perda validação = 1.1648\n",
      "Época 121: perda treino = 1.0880, perda validação = 1.1644\n",
      "Época 122: perda treino = 1.0873, perda validação = 1.1639\n",
      "Época 123: perda treino = 1.0866, perda validação = 1.1634\n",
      "Época 124: perda treino = 1.0860, perda validação = 1.1629\n",
      "Época 125: perda treino = 1.0853, perda validação = 1.1625\n",
      "Época 126: perda treino = 1.0846, perda validação = 1.1620\n",
      "Época 127: perda treino = 1.0839, perda validação = 1.1616\n",
      "Época 128: perda treino = 1.0833, perda validação = 1.1611\n",
      "Época 129: perda treino = 1.0826, perda validação = 1.1607\n",
      "Época 130: perda treino = 1.0820, perda validação = 1.1602\n",
      "Época 131: perda treino = 1.0813, perda validação = 1.1598\n",
      "Época 132: perda treino = 1.0807, perda validação = 1.1594\n",
      "Época 133: perda treino = 1.0800, perda validação = 1.1590\n",
      "Época 134: perda treino = 1.0794, perda validação = 1.1585\n",
      "Época 135: perda treino = 1.0788, perda validação = 1.1581\n",
      "Época 136: perda treino = 1.0782, perda validação = 1.1577\n",
      "Época 137: perda treino = 1.0775, perda validação = 1.1573\n",
      "Época 138: perda treino = 1.0769, perda validação = 1.1569\n",
      "Época 139: perda treino = 1.0763, perda validação = 1.1565\n",
      "Época 140: perda treino = 1.0757, perda validação = 1.1561\n",
      "Época 141: perda treino = 1.0751, perda validação = 1.1557\n",
      "Época 142: perda treino = 1.0745, perda validação = 1.1553\n",
      "Época 143: perda treino = 1.0740, perda validação = 1.1549\n",
      "Época 144: perda treino = 1.0734, perda validação = 1.1545\n",
      "Época 145: perda treino = 1.0728, perda validação = 1.1542\n",
      "Época 146: perda treino = 1.0722, perda validação = 1.1538\n",
      "Época 147: perda treino = 1.0716, perda validação = 1.1534\n",
      "Época 148: perda treino = 1.0711, perda validação = 1.1531\n",
      "Época 149: perda treino = 1.0705, perda validação = 1.1527\n",
      "Época 150: perda treino = 1.0700, perda validação = 1.1523\n",
      "Época 151: perda treino = 1.0694, perda validação = 1.1520\n",
      "Época 152: perda treino = 1.0689, perda validação = 1.1516\n",
      "Época 153: perda treino = 1.0683, perda validação = 1.1513\n",
      "Época 154: perda treino = 1.0678, perda validação = 1.1509\n",
      "Época 155: perda treino = 1.0673, perda validação = 1.1506\n",
      "Época 156: perda treino = 1.0667, perda validação = 1.1503\n",
      "Época 157: perda treino = 1.0662, perda validação = 1.1499\n",
      "Época 158: perda treino = 1.0657, perda validação = 1.1496\n",
      "Época 159: perda treino = 1.0652, perda validação = 1.1493\n",
      "Época 160: perda treino = 1.0646, perda validação = 1.1489\n",
      "Época 161: perda treino = 1.0641, perda validação = 1.1486\n",
      "Época 162: perda treino = 1.0636, perda validação = 1.1483\n",
      "Época 163: perda treino = 1.0631, perda validação = 1.1480\n",
      "Época 164: perda treino = 1.0626, perda validação = 1.1477\n",
      "Época 165: perda treino = 1.0621, perda validação = 1.1474\n",
      "Época 166: perda treino = 1.0616, perda validação = 1.1471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época 167: perda treino = 1.0612, perda validação = 1.1468\n",
      "Época 168: perda treino = 1.0607, perda validação = 1.1465\n",
      "Época 169: perda treino = 1.0602, perda validação = 1.1462\n",
      "Época 170: perda treino = 1.0597, perda validação = 1.1459\n",
      "Época 171: perda treino = 1.0593, perda validação = 1.1456\n",
      "Época 172: perda treino = 1.0588, perda validação = 1.1453\n",
      "Época 173: perda treino = 1.0583, perda validação = 1.1450\n",
      "Época 174: perda treino = 1.0579, perda validação = 1.1447\n",
      "Época 175: perda treino = 1.0574, perda validação = 1.1445\n",
      "Época 176: perda treino = 1.0570, perda validação = 1.1442\n",
      "Época 177: perda treino = 1.0565, perda validação = 1.1439\n",
      "Época 178: perda treino = 1.0561, perda validação = 1.1436\n",
      "Época 179: perda treino = 1.0556, perda validação = 1.1434\n",
      "Época 180: perda treino = 1.0552, perda validação = 1.1431\n",
      "Época 181: perda treino = 1.0548, perda validação = 1.1428\n",
      "Época 182: perda treino = 1.0543, perda validação = 1.1426\n",
      "Época 183: perda treino = 1.0539, perda validação = 1.1423\n",
      "Época 184: perda treino = 1.0535, perda validação = 1.1421\n",
      "Época 185: perda treino = 1.0530, perda validação = 1.1418\n",
      "Época 186: perda treino = 1.0526, perda validação = 1.1416\n",
      "Época 187: perda treino = 1.0522, perda validação = 1.1413\n",
      "Época 188: perda treino = 1.0518, perda validação = 1.1411\n",
      "Época 189: perda treino = 1.0514, perda validação = 1.1409\n",
      "Época 190: perda treino = 1.0510, perda validação = 1.1406\n",
      "Época 191: perda treino = 1.0506, perda validação = 1.1404\n",
      "Época 192: perda treino = 1.0502, perda validação = 1.1402\n",
      "Época 193: perda treino = 1.0498, perda validação = 1.1399\n",
      "Época 194: perda treino = 1.0494, perda validação = 1.1397\n",
      "Época 195: perda treino = 1.0490, perda validação = 1.1395\n",
      "Época 196: perda treino = 1.0486, perda validação = 1.1393\n",
      "Época 197: perda treino = 1.0482, perda validação = 1.1390\n",
      "Época 198: perda treino = 1.0479, perda validação = 1.1388\n",
      "Época 199: perda treino = 1.0475, perda validação = 1.1386\n",
      "Época 200: perda treino = 1.0471, perda validação = 1.1384\n",
      "Época 201: perda treino = 1.0467, perda validação = 1.1382\n",
      "Época 202: perda treino = 1.0464, perda validação = 1.1380\n",
      "Época 203: perda treino = 1.0460, perda validação = 1.1378\n",
      "Época 204: perda treino = 1.0457, perda validação = 1.1376\n",
      "Época 205: perda treino = 1.0453, perda validação = 1.1374\n",
      "Época 206: perda treino = 1.0449, perda validação = 1.1372\n",
      "Época 207: perda treino = 1.0446, perda validação = 1.1370\n",
      "Época 208: perda treino = 1.0442, perda validação = 1.1368\n",
      "Época 209: perda treino = 1.0439, perda validação = 1.1366\n",
      "Época 210: perda treino = 1.0435, perda validação = 1.1364\n",
      "Época 211: perda treino = 1.0432, perda validação = 1.1362\n",
      "Época 212: perda treino = 1.0429, perda validação = 1.1360\n",
      "Época 213: perda treino = 1.0425, perda validação = 1.1358\n",
      "Época 214: perda treino = 1.0422, perda validação = 1.1357\n",
      "Época 215: perda treino = 1.0419, perda validação = 1.1355\n",
      "Época 216: perda treino = 1.0415, perda validação = 1.1353\n",
      "Época 217: perda treino = 1.0412, perda validação = 1.1351\n",
      "Época 218: perda treino = 1.0409, perda validação = 1.1349\n",
      "Época 219: perda treino = 1.0406, perda validação = 1.1348\n",
      "Época 220: perda treino = 1.0402, perda validação = 1.1346\n",
      "Época 221: perda treino = 1.0399, perda validação = 1.1344\n",
      "Época 222: perda treino = 1.0396, perda validação = 1.1343\n",
      "Época 223: perda treino = 1.0393, perda validação = 1.1341\n",
      "Época 224: perda treino = 1.0390, perda validação = 1.1339\n",
      "Época 225: perda treino = 1.0387, perda validação = 1.1338\n",
      "Época 226: perda treino = 1.0384, perda validação = 1.1336\n",
      "Época 227: perda treino = 1.0381, perda validação = 1.1335\n",
      "Época 228: perda treino = 1.0378, perda validação = 1.1333\n",
      "Época 229: perda treino = 1.0375, perda validação = 1.1332\n",
      "Época 230: perda treino = 1.0372, perda validação = 1.1330\n",
      "Época 231: perda treino = 1.0369, perda validação = 1.1329\n",
      "Época 232: perda treino = 1.0366, perda validação = 1.1327\n",
      "Época 233: perda treino = 1.0363, perda validação = 1.1326\n",
      "Época 234: perda treino = 1.0360, perda validação = 1.1324\n",
      "Época 235: perda treino = 1.0357, perda validação = 1.1323\n",
      "Época 236: perda treino = 1.0355, perda validação = 1.1321\n",
      "Época 237: perda treino = 1.0352, perda validação = 1.1320\n",
      "Época 238: perda treino = 1.0349, perda validação = 1.1319\n",
      "Época 239: perda treino = 1.0346, perda validação = 1.1317\n",
      "Época 240: perda treino = 1.0344, perda validação = 1.1316\n",
      "Época 241: perda treino = 1.0341, perda validação = 1.1315\n",
      "Época 242: perda treino = 1.0338, perda validação = 1.1313\n",
      "Época 243: perda treino = 1.0336, perda validação = 1.1312\n",
      "Época 244: perda treino = 1.0333, perda validação = 1.1311\n",
      "Época 245: perda treino = 1.0330, perda validação = 1.1309\n",
      "Época 246: perda treino = 1.0328, perda validação = 1.1308\n",
      "Época 247: perda treino = 1.0325, perda validação = 1.1307\n",
      "Época 248: perda treino = 1.0323, perda validação = 1.1306\n",
      "Época 249: perda treino = 1.0320, perda validação = 1.1304\n",
      "Época 250: perda treino = 1.0317, perda validação = 1.1303\n",
      "Época 251: perda treino = 1.0315, perda validação = 1.1302\n",
      "Época 252: perda treino = 1.0312, perda validação = 1.1301\n",
      "Época 253: perda treino = 1.0310, perda validação = 1.1300\n",
      "Época 254: perda treino = 1.0308, perda validação = 1.1299\n",
      "Época 255: perda treino = 1.0305, perda validação = 1.1298\n",
      "Época 256: perda treino = 1.0303, perda validação = 1.1296\n",
      "Época 257: perda treino = 1.0300, perda validação = 1.1295\n",
      "Época 258: perda treino = 1.0298, perda validação = 1.1294\n",
      "Época 259: perda treino = 1.0296, perda validação = 1.1293\n",
      "Época 260: perda treino = 1.0293, perda validação = 1.1292\n",
      "Época 261: perda treino = 1.0291, perda validação = 1.1291\n",
      "Época 262: perda treino = 1.0289, perda validação = 1.1290\n",
      "Época 263: perda treino = 1.0287, perda validação = 1.1289\n",
      "Época 264: perda treino = 1.0284, perda validação = 1.1288\n",
      "Época 265: perda treino = 1.0282, perda validação = 1.1287\n",
      "Época 266: perda treino = 1.0280, perda validação = 1.1286\n",
      "Época 267: perda treino = 1.0278, perda validação = 1.1285\n",
      "Época 268: perda treino = 1.0275, perda validação = 1.1284\n",
      "Época 269: perda treino = 1.0273, perda validação = 1.1283\n",
      "Época 270: perda treino = 1.0271, perda validação = 1.1282\n",
      "Época 271: perda treino = 1.0269, perda validação = 1.1281\n",
      "Época 272: perda treino = 1.0267, perda validação = 1.1281\n",
      "Época 273: perda treino = 1.0265, perda validação = 1.1280\n",
      "Parada antecipada na época 273 com perda da validação = 1.1280\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCAS = 1000\n",
    "paciencia = 10\n",
    "min_delta = 0.001\n",
    "melhor_loss = float(\"inf\")\n",
    "contador_paciencia = 0\n",
    "\n",
    "minha_mlp.train()\n",
    "\n",
    "for epoca in range(NUM_EPOCAS):\n",
    "    y_pred = minha_mlp(X_train_tensor)\n",
    "\n",
    "    otimizador.zero_grad()\n",
    "\n",
    "    loss = fn_perda(y_pred, y_train_tensor)\n",
    "    \n",
    "    y_pred_val = minha_mlp(X_val_tensor)\n",
    "    loss_val = fn_perda(y_pred_val, y_val_tensor)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    otimizador.step()\n",
    "    \n",
    "    print(f\"Época {epoca}: perda treino = {loss.item():.4f}, perda validação = {loss_val.item():.4f}\")\n",
    "\n",
    "    # === EARLY STOPPING COM BASE NA VALIDAÇÃO ===\n",
    "    if loss_val.item() < melhor_loss - min_delta:\n",
    "        melhor_loss = loss_val.item()\n",
    "        contador_paciencia = 0\n",
    "    else:\n",
    "        contador_paciencia += 1\n",
    "        if contador_paciencia >= paciencia:\n",
    "            print(f\"Parada antecipada na época {epoca} com perda da validação = {loss_val.item():.4f}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7eef7f",
   "metadata": {},
   "source": [
    "## Conclusão\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a551b581",
   "metadata": {},
   "source": [
    "Em síntese, este notebook demonstrou como o early stopping pode atuar como um critério de parada inteligente em redes neurais, interrompendo o processo assim que a métrica de validação deixa de melhorar. Observamos, que em geral há um ponto em que o modelo começa a dar overfitting no conjunto de treino e a performance em validação estagna ou piora; ao aplicar early stopping, capturamos exatamente a iteração ótima, garantindo maior capacidade do modelo.\n",
    "\n",
    "Além do ganho em qualidade do modelo, o uso desse mecanismo também trouxe economia de tempo de treinamento, evitando ciclos extras de ajuste que não trazem benefício de validação. Como resultado, conseguimos um modelo com menor custo computacional e reduzimos o risco de overfitting sem precisar adivinhar o número exato de épocas. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e68b601",
   "metadata": {},
   "source": [
    "## Referências\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3586a91e",
   "metadata": {},
   "source": [
    "- Cassar, Daniel R. - Material da disciplina de Redes Neurais e Algoritmos genéticos. 2025\n",
    "- [medium](https://cyborgcodes.medium.com/what-is-early-stopping-in-deep-learning-eeb1e710a3cf)\n",
    "- [dataset](https://www.kaggle.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac08dbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
